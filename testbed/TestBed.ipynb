{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d987a662",
   "metadata": {},
   "source": [
    "## Getting Started"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-25T12:42:17.954140Z",
     "start_time": "2024-09-25T12:42:16.574058Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import io\n",
    "import chardet\n",
    "\n",
    "def detect_encoding(file_path):\n",
    "    with open(file_path, 'rb') as file:\n",
    "        raw = file.read()\n",
    "    return chardet.detect(raw)['encoding']\n",
    "\n",
    "def process_bike_rental_data(input_file, output_file):\n",
    "    # Detect file encoding\n",
    "    file_encoding = detect_encoding(input_file)\n",
    "    print(f\"Detected file encoding: {file_encoding}\")\n",
    "\n",
    "    # Read the file content\n",
    "    with open(input_file, 'r', encoding=file_encoding, errors='replace') as file:\n",
    "        content = file.read()\n",
    "\n",
    "    # Use StringIO to create a file-like object from the string content\n",
    "    string_io = io.StringIO(content)\n",
    "    \n",
    "    # Read the CSV file using pandas\n",
    "    df = pd.read_csv(string_io)\n",
    "\n",
    "    print(\"Original columns:\", df.columns.tolist())\n",
    "    print(\"Original shape:\", df.shape)\n",
    "    \n",
    "    # Discard categorical columns\n",
    "    columns_to_drop = ['Seasons', 'Holiday', 'Functioning Day']\n",
    "    df = df.drop(columns=columns_to_drop)\n",
    "    \n",
    "    # Convert Date and Hour to datetime\n",
    "    df['Date'] = pd.to_datetime(df['Date'], format='%d/%m/%Y')\n",
    "    df['Date'] = df['Date'] + pd.to_timedelta(df['Hour'], unit='h')\n",
    "    \n",
    "    # Drop the Hour column as it's now integrated into Date\n",
    "    df = df.drop(columns=['Hour'])\n",
    "    \n",
    "    # Reorder columns to have Date first\n",
    "    columns = ['Date'] + [col for col in df.columns if col != 'Date']\n",
    "    df = df[columns]\n",
    "    \n",
    "    # Save the processed DataFrame to a new CSV file\n",
    "    df.to_csv(output_file, index=False)\n",
    "    \n",
    "    print(f\"Processed data saved to {output_file}\")\n",
    "    print(\"Final columns:\", df.columns.tolist())\n",
    "    print(\"Final shape:\", df.shape)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Example usage:\n",
    "input_file = '/Users/moji/PyTSF-MfG/data/SeoulBikeData.csv'  # Replace with your input file name\n",
    "output_file = '/Users/moji/PyTSF-MfG/data/SeoulBikeData_processed.csv'  # Replace with your desired output file name\n",
    "\n",
    "processed_df = process_bike_rental_data(input_file, output_file)\n",
    "\n",
    "# Print the first few rows of the processed DataFrame\n",
    "print(\"\\nFirst few rows of the processed data:\")\n",
    "print(processed_df.head())\n",
    "\n",
    "# Print summary statistics of the numeric data\n",
    "print(\"\\nSummary statistics of the numeric data:\")\n",
    "print(processed_df.describe())"
   ],
   "id": "efa37995d7a1653",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected file encoding: ISO-8859-1\n",
      "Original columns: ['Date', 'Rented Bike Count', 'Hour', 'Temperature(°C)', 'Humidity(%)', 'Wind speed (m/s)', 'Visibility (10m)', 'Dew point temperature(°C)', 'Solar Radiation (MJ/m2)', 'Rainfall(mm)', 'Snowfall (cm)', 'Seasons', 'Holiday', 'Functioning Day']\n",
      "Original shape: (8760, 14)\n",
      "Processed data saved to /Users/moji/PyTSF-MfG/data/SeoulBikeData_processed.csv\n",
      "Final columns: ['Date', 'Rented Bike Count', 'Temperature(°C)', 'Humidity(%)', 'Wind speed (m/s)', 'Visibility (10m)', 'Dew point temperature(°C)', 'Solar Radiation (MJ/m2)', 'Rainfall(mm)', 'Snowfall (cm)']\n",
      "Final shape: (8760, 10)\n",
      "\n",
      "First few rows of the processed data:\n",
      "                 Date  Rented Bike Count  Temperature(°C)  Humidity(%)  \\\n",
      "0 2017-12-01 00:00:00                254             -5.2           37   \n",
      "1 2017-12-01 01:00:00                204             -5.5           38   \n",
      "2 2017-12-01 02:00:00                173             -6.0           39   \n",
      "3 2017-12-01 03:00:00                107             -6.2           40   \n",
      "4 2017-12-01 04:00:00                 78             -6.0           36   \n",
      "\n",
      "   Wind speed (m/s)  Visibility (10m)  Dew point temperature(°C)  \\\n",
      "0               2.2              2000                      -17.6   \n",
      "1               0.8              2000                      -17.6   \n",
      "2               1.0              2000                      -17.7   \n",
      "3               0.9              2000                      -17.6   \n",
      "4               2.3              2000                      -18.6   \n",
      "\n",
      "   Solar Radiation (MJ/m2)  Rainfall(mm)  Snowfall (cm)  \n",
      "0                      0.0           0.0            0.0  \n",
      "1                      0.0           0.0            0.0  \n",
      "2                      0.0           0.0            0.0  \n",
      "3                      0.0           0.0            0.0  \n",
      "4                      0.0           0.0            0.0  \n",
      "\n",
      "Summary statistics of the numeric data:\n",
      "                      Date  Rented Bike Count  Temperature(°C)  Humidity(%)  \\\n",
      "count                 8760        8760.000000      8760.000000  8760.000000   \n",
      "mean   2018-06-01 11:30:00         704.602055        12.882922    58.226256   \n",
      "min    2017-12-01 00:00:00           0.000000       -17.800000     0.000000   \n",
      "25%    2018-03-02 05:45:00         191.000000         3.500000    42.000000   \n",
      "50%    2018-06-01 11:30:00         504.500000        13.700000    57.000000   \n",
      "75%    2018-08-31 17:15:00        1065.250000        22.500000    74.000000   \n",
      "max    2018-11-30 23:00:00        3556.000000        39.400000    98.000000   \n",
      "std                    NaN         644.997468        11.944825    20.362413   \n",
      "\n",
      "       Wind speed (m/s)  Visibility (10m)  Dew point temperature(°C)  \\\n",
      "count       8760.000000       8760.000000                8760.000000   \n",
      "mean           1.724909       1436.825799                   4.073813   \n",
      "min            0.000000         27.000000                 -30.600000   \n",
      "25%            0.900000        940.000000                  -4.700000   \n",
      "50%            1.500000       1698.000000                   5.100000   \n",
      "75%            2.300000       2000.000000                  14.800000   \n",
      "max            7.400000       2000.000000                  27.200000   \n",
      "std            1.036300        608.298712                  13.060369   \n",
      "\n",
      "       Solar Radiation (MJ/m2)  Rainfall(mm)  Snowfall (cm)  \n",
      "count              8760.000000   8760.000000    8760.000000  \n",
      "mean                  0.569111      0.148687       0.075068  \n",
      "min                   0.000000      0.000000       0.000000  \n",
      "25%                   0.000000      0.000000       0.000000  \n",
      "50%                   0.010000      0.000000       0.000000  \n",
      "75%                   0.930000      0.000000       0.000000  \n",
      "max                   3.520000     35.000000       8.800000  \n",
      "std                   0.868746      1.128193       0.436746  \n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "import numpy as np\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv('/Users/moji/PyTSF-MfG/data/energydata_complete.csv')\n",
    "\n",
    "# Convert 'Date_Time' to datetime for time series analysis\n",
    "df['Timestamp'] = pd.to_datetime(df['date'], format='%Y-%m-%d %H:%M:%S')\n",
    "df.set_index('Timestamp', inplace=True)\n",
    "\n",
    "# Select only numeric columns for correlation\n",
    "numeric_columns = df.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "# Time Series Visualizations\n",
    "plt.figure(figsize=(20, 15))\n",
    "\n",
    "# 1. Time Series of Energy Usage\n",
    "plt.subplot(3, 1, 1)\n",
    "df['Press_mm_hg'].plot()\n",
    "plt.title('Time Series of Energy Usage')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Usage (kWh)')\n",
    "\n",
    "# 2. Rolling statistics\n",
    "window_size = 24 * 7  # One week\n",
    "rolling_mean = df['Press_mm_hg'].rolling(window=window_size).mean()\n",
    "rolling_std = df['Press_mm_hg'].rolling(window=window_size).std()\n",
    "\n",
    "plt.subplot(3, 1, 2)\n",
    "plt.plot(df.index, df['Press_mm_hg'], label='Energy Usage')\n",
    "plt.plot(df.index, rolling_mean, label=f'Rolling Mean (window={window_size})')\n",
    "plt.plot(df.index, rolling_std, label=f'Rolling Std (window={window_size})')\n",
    "plt.title('Energy Usage - Rolling Statistics')\n",
    "plt.legend()\n",
    "\n",
    "# 3. Histogram of Energy Usage\n",
    "plt.subplot(3, 1, 3)\n",
    "df['Press_mm_hg'].hist(bins=50)\n",
    "plt.title('Distribution of Energy Usage')\n",
    "plt.xlabel('Usage (kWh)')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Seasonal Decomposition\n",
    "decomposition = seasonal_decompose(df['Press_mm_hg'], model='additive', period=24*7)  # Weekly seasonality\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "decomposition.plot()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Autocorrelation and Partial Autocorrelation\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 8))\n",
    "plot_acf(df['Press_mm_hg'].dropna(), ax=ax1, lags=24*7)\n",
    "plot_pacf(df['Press_mm_hg'].dropna(), ax=ax2, lags=24*7)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Correlation with other numeric variables\n",
    "correlation = df[numeric_columns].corr()['Press_mm_hg'].sort_values(ascending=False)\n",
    "print(\"Correlation with Energy Usage:\")\n",
    "print(correlation)\n",
    "\n",
    "# Plot correlations\n",
    "plt.figure(figsize=(10, 6))\n",
    "correlation.plot(kind='bar')\n",
    "plt.title('Feature Correlation with Energy Usage')\n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('Correlation Coefficient')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Time Series Forecasting Problem Definition\n",
    "print(\"\\nTime Series Forecasting Problem Definition:\")\n",
    "print(\"Target Variable: Usage_kWh\")\n",
    "print(\"Features: Historical values of Usage_kWh, and other numeric features\")\n",
    "print(\"Task: Predict future values of Usage_kWh based on historical data and other features\")\n",
    "\n",
    "# Basic statistics\n",
    "print(\"\\nBasic Statistics for Energy Usage:\")\n",
    "print(df['Press_mm_hg'].describe())\n",
    "\n",
    "# Check for stationarity\n",
    "result = adfuller(df['Press_mm_hg'].dropna())\n",
    "print('\\nAugmented Dickey-Fuller Test:')\n",
    "print(f'ADF Statistic: {result[0]}')\n",
    "print(f'p-value: {result[1]}')\n",
    "print('Critical Values:')\n",
    "for key, value in result[4].items():\n",
    "    print(f'\\t{key}: {value}')\n",
    "\n",
    "# Print data types of all columns\n",
    "print(\"\\nData Types of Columns:\")\n",
    "print(df.dtypes)\n",
    "\n",
    "# Print unique values in categorical columns\n",
    "for column in df.select_dtypes(include=['object']).columns:\n",
    "    print(f\"\\nUnique values in {column}:\")\n",
    "    print(df[column].unique())\n",
    "\n",
    "# Additional analyses\n",
    "\n",
    "# 1. Average energy usage by day of week\n",
    "df['Day_Of_Week'] = df.index.day_name()\n",
    "plt.figure(figsize=(10, 6))\n",
    "df.groupby('Day_Of_Week')['Press_mm_hg'].mean().plot(kind='bar')\n",
    "plt.title('Average Energy Usage by Day of Week')\n",
    "plt.ylabel('Average Usage (kWh)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 2. Average energy usage by hour of day\n",
    "df['Hour'] = df.index.hour\n",
    "plt.figure(figsize=(12, 6))\n",
    "df.groupby('Hour')['Press_mm_hg'].mean().plot()\n",
    "plt.title('Average Energy Usage by Hour of Day')\n",
    "plt.xlabel('Hour')\n",
    "plt.ylabel('Average Usage (kWh)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 3. Energy usage heatmap (Day of Week vs Hour)\n",
    "plt.figure(figsize=(12, 8))\n",
    "usage_pivot = df.pivot_table(values='Press_mm_hg', index='Day_Of_Week', columns='Hour', aggfunc='mean')\n",
    "sns.heatmap(usage_pivot, cmap='YlOrRd')\n",
    "plt.title('Energy Usage Heatmap: Day of Week vs Hour')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 4. Correlation heatmap for all numeric variables\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(df[numeric_columns].corr(), annot=True, cmap='coolwarm', linewidths=0.5)\n",
    "plt.title('Correlation Heatmap of Numeric Variables')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "c7481448c31422e4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming df is your DataFrame with the 'Timestamp' index\n",
    "# If not, uncomment and run these lines:\n",
    "# df = pd.read_csv('/path/to/your/energy_consumption.csv')\n",
    "# df['Timestamp'] = pd.to_datetime(df['Date_Time'], format='%d-%m-%Y %H:%M')\n",
    "# df.set_index('Timestamp', inplace=True)\n",
    "\n",
    "# Calculate the time difference between consecutive rows\n",
    "time_diff = df.index.to_series().diff()\n",
    "\n",
    "# Get the most common time difference\n",
    "most_common_diff = time_diff.mode().iloc[0]\n",
    "\n",
    "print(f\"The most common time difference between data points is: {most_common_diff}\")\n",
    "\n",
    "# Check if all time differences are the same\n",
    "if (time_diff == most_common_diff).all():\n",
    "    print(\"The dataset has a consistent frequency.\")\n",
    "else:\n",
    "    print(\"The dataset has some inconsistent time intervals.\")\n",
    "\n",
    "# Count the occurrences of each unique time difference\n",
    "diff_counts = time_diff.value_counts()\n",
    "\n",
    "print(\"\\nCounts of unique time differences:\")\n",
    "print(diff_counts)\n",
    "\n",
    "# Calculate the percentage of the most common difference\n",
    "most_common_percentage = (diff_counts.iloc[0] / len(df)) * 100\n",
    "\n",
    "print(f\"\\nPercentage of data points with the most common time difference: {most_common_percentage:.2f}%\")\n",
    "\n",
    "# If there are inconsistencies, show some examples\n",
    "if len(diff_counts) > 1:\n",
    "    print(\"\\nExamples of inconsistent time intervals:\")\n",
    "    inconsistent = df[time_diff != most_common_diff]\n",
    "    print(inconsistent.head())\n",
    "\n",
    "# Visualize the distribution of time differences\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "time_diff.value_counts().plot(kind='bar')\n",
    "plt.title('Distribution of Time Differences')\n",
    "plt.xlabel('Time Difference')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "10cf68d26c5a8d4a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "import numpy as np\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv('/Users/moji/PyTSF-MfG/data/ai4i2020.csv')\n",
    "\n",
    "# Convert 'UDI' to datetime for time series analysis\n",
    "df['Timestamp'] = pd.to_datetime(df['UDI'], unit='s')  # Assuming UDI represents seconds\n",
    "df.set_index('Timestamp', inplace=True)\n",
    "\n",
    "# Select only numeric columns for correlation\n",
    "numeric_columns = df.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "# Time Series Visualizations\n",
    "plt.figure(figsize=(20, 15))\n",
    "\n",
    "# 1. Time Series of Rotational Speed\n",
    "plt.subplot(3, 1, 1)\n",
    "df['Rotational speed [rpm]'].plot()\n",
    "plt.title('Time Series of Rotational Speed')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Rotational Speed [rpm]')\n",
    "\n",
    "# 2. Rolling statistics\n",
    "window_size = 30\n",
    "rolling_mean = df['Rotational speed [rpm]'].rolling(window=window_size).mean()\n",
    "rolling_std = df['Rotational speed [rpm]'].rolling(window=window_size).std()\n",
    "\n",
    "plt.subplot(3, 1, 2)\n",
    "plt.plot(df.index, df['Rotational speed [rpm]'], label='Rotational Speed')\n",
    "plt.plot(df.index, rolling_mean, label=f'Rolling Mean (window={window_size})')\n",
    "plt.plot(df.index, rolling_std, label=f'Rolling Std (window={window_size})')\n",
    "plt.title('Rotational Speed - Rolling Statistics')\n",
    "plt.legend()\n",
    "\n",
    "# 3. Histogram of Rotational Speed\n",
    "plt.subplot(3, 1, 3)\n",
    "df['Rotational speed [rpm]'].hist(bins=50)\n",
    "plt.title('Distribution of Rotational Speed')\n",
    "plt.xlabel('Rotational Speed [rpm]')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Seasonal Decomposition\n",
    "decomposition = seasonal_decompose(df['Rotational speed [rpm]'], model='additive', period=24)  # Assuming hourly data\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "decomposition.plot()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Autocorrelation and Partial Autocorrelation\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 8))\n",
    "plot_acf(df['Rotational speed [rpm]'].dropna(), ax=ax1, lags=50)\n",
    "plot_pacf(df['Rotational speed [rpm]'].dropna(), ax=ax2, lags=50)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Correlation with other numeric variables\n",
    "correlation = df[numeric_columns].corr()['Rotational speed [rpm]'].sort_values(ascending=False)\n",
    "print(\"Correlation with Rotational Speed:\")\n",
    "print(correlation)\n",
    "\n",
    "# Plot correlations\n",
    "plt.figure(figsize=(10, 6))\n",
    "correlation.plot(kind='bar')\n",
    "plt.title('Feature Correlation with Rotational Speed')\n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('Correlation Coefficient')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Time Series Forecasting Problem Definition\n",
    "print(\"\\nTime Series Forecasting Problem Definition:\")\n",
    "print(\"Target Variable: Rotational speed [rpm]\")\n",
    "print(\"Features: Historical values of Rotational speed [rpm], and other numeric features\")\n",
    "print(\"Task: Predict future values of Rotational speed [rpm] based on historical data and other features\")\n",
    "print(\"Potential Models: ARIMA, SARIMA, Prophet, or machine learning models like Random Forest, XGBoost, or LSTM\")\n",
    "\n",
    "# Basic statistics\n",
    "print(\"\\nBasic Statistics for Rotational Speed:\")\n",
    "print(df['Rotational speed [rpm]'].describe())\n",
    "\n",
    "# Check for stationarity\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "result = adfuller(df['Rotational speed [rpm]'].dropna())\n",
    "print('\\nAugmented Dickey-Fuller Test:')\n",
    "print(f'ADF Statistic: {result[0]}')\n",
    "print(f'p-value: {result[1]}')\n",
    "print('Critical Values:')\n",
    "for key, value in result[4].items():\n",
    "    print(f'\\t{key}: {value}')\n",
    "\n",
    "# Print data types of all columns\n",
    "print(\"\\nData Types of Columns:\")\n",
    "print(df.dtypes)\n",
    "\n",
    "# Print unique values in non-numeric columns\n",
    "for column in df.select_dtypes(exclude=[np.number]).columns:\n",
    "    print(f\"\\nUnique values in {column}:\")\n",
    "    print(df[column].unique())"
   ],
   "id": "3363facd77127761",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "import numpy as np\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv('/Users/moji/Downloads/ai4i2020.csv')\n",
    "\n",
    "# Convert 'UDI' to datetime for time series analysis\n",
    "df['Timestamp'] = pd.to_datetime(df['UDI'], unit='s')  # Assuming UDI represents seconds\n",
    "df.set_index('Timestamp', inplace=True)\n",
    "\n",
    "# Select variables for analysis\n",
    "variables = ['Rotational speed [rpm]', 'Air temperature [K]', 'Process temperature [K]']\n",
    "\n",
    "# Select only numeric columns for correlation\n",
    "numeric_columns = df.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "# Time Series Visualizations\n",
    "fig, axes = plt.subplots(3, 1, figsize=(20, 15))\n",
    "fig.suptitle('Time Series Comparison', fontsize=16)\n",
    "\n",
    "for i, var in enumerate(variables):\n",
    "    axes[i].plot(df.index, df[var])\n",
    "    axes[i].set_title(f'Time Series of {var}')\n",
    "    axes[i].set_xlabel('Time')\n",
    "    axes[i].set_ylabel(var)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Rolling Statistics\n",
    "window_size = 30\n",
    "fig, axes = plt.subplots(3, 1, figsize=(20, 15))\n",
    "fig.suptitle('Rolling Statistics Comparison', fontsize=16)\n",
    "\n",
    "for i, var in enumerate(variables):\n",
    "    rolling_mean = df[var].rolling(window=window_size).mean()\n",
    "    rolling_std = df[var].rolling(window=window_size).std()\n",
    "    \n",
    "    axes[i].plot(df.index, df[var], label=var)\n",
    "    axes[i].plot(df.index, rolling_mean, label=f'Rolling Mean (window={window_size})')\n",
    "    axes[i].plot(df.index, rolling_std, label=f'Rolling Std (window={window_size})')\n",
    "    axes[i].set_title(f'{var} - Rolling Statistics')\n",
    "    axes[i].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Histograms\n",
    "fig, axes = plt.subplots(1, 3, figsize=(20, 5))\n",
    "fig.suptitle('Distribution Comparison', fontsize=16)\n",
    "\n",
    "for i, var in enumerate(variables):\n",
    "    axes[i].hist(df[var], bins=50)\n",
    "    axes[i].set_title(f'Distribution of {var}')\n",
    "    axes[i].set_xlabel(var)\n",
    "    axes[i].set_ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Seasonal Decomposition\n",
    "for var in variables:\n",
    "    decomposition = seasonal_decompose(df[var], model='additive', period=24)  # Assuming hourly data\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    decomposition.plot()\n",
    "    plt.suptitle(f'Seasonal Decomposition of {var}', fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Autocorrelation and Partial Autocorrelation\n",
    "for var in variables:\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 8))\n",
    "    plot_acf(df[var].dropna(), ax=ax1, lags=50)\n",
    "    plot_pacf(df[var].dropna(), ax=ax2, lags=50)\n",
    "    plt.suptitle(f'ACF and PACF of {var}', fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Correlation Analysis\n",
    "correlation_matrix = df[variables].corr()\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')\n",
    "plt.title('Correlation Heatmap')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Time Series Forecasting Problem Definition\n",
    "print(\"\\nTime Series Forecasting Problem Definition:\")\n",
    "for var in variables:\n",
    "    print(f\"\\nTarget Variable: {var}\")\n",
    "    print(f\"Features: Historical values of {var}, and other relevant numeric variables\")\n",
    "    print(f\"Task: Predict future values of {var} based on historical data and other features\")\n",
    "    print(\"Potential Models: ARIMA, SARIMA, Prophet, or machine learning models like Random Forest, XGBoost, or LSTM\")\n",
    "\n",
    "# Basic statistics and stationarity test\n",
    "for var in variables:\n",
    "    print(f\"\\nBasic Statistics for {var}:\")\n",
    "    print(df[var].describe())\n",
    "    \n",
    "    result = adfuller(df[var].dropna())\n",
    "    print(f'\\nAugmented Dickey-Fuller Test for {var}:')\n",
    "    print(f'ADF Statistic: {result[0]}')\n",
    "    print(f'p-value: {result[1]}')\n",
    "    print('Critical Values:')\n",
    "    for key, value in result[4].items():\n",
    "        print(f'\\t{key}: {value}')\n",
    "\n",
    "# Correlation with other numeric variables\n",
    "for var in variables:\n",
    "    correlation = df[numeric_columns].corr()[var].sort_values(ascending=False)\n",
    "    print(f\"\\nCorrelation with {var}:\")\n",
    "    print(correlation)\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    correlation.plot(kind='bar')\n",
    "    plt.title(f'Feature Correlation with {var}')\n",
    "    plt.xlabel('Features')\n",
    "    plt.ylabel('Correlation Coefficient')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Print data types of all columns\n",
    "print(\"\\nData Types of Columns:\")\n",
    "print(df.dtypes)\n",
    "\n",
    "# Print unique values in non-numeric columns\n",
    "for column in df.select_dtypes(exclude=[np.number]).columns:\n",
    "    print(f\"\\nUnique values in {column}:\")\n",
    "    print(df[column].unique())"
   ],
   "id": "3fce61b5b857e3c1",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c3f4409b",
   "metadata": {},
   "source": [
    "from neuralforecast.utils import AirPassengersDF\n",
    "\n",
    "Y_df = AirPassengersDF # Defined in neuralforecast.utils\n",
    "Y_df.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "82434573",
   "metadata": {},
   "source": [
    "### Important:\n",
    "* DataFrames must include all ['unique_id', 'ds', 'y'] columns. Make sure y column does not have missing or non-numeric values."
   ]
  },
  {
   "cell_type": "code",
   "id": "da5abb8a",
   "metadata": {},
   "source": [
    "from neuralforecast import NeuralForecast\n",
    "from neuralforecast.models import LSTM, NHITS, RNN"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "98406e81",
   "metadata": {},
   "source": [
    "horizon = 12\n",
    "\n",
    "# Try different hyperparmeters to improve accuracy.\n",
    "models = [LSTM(h=horizon,                    # Forecast horizon\n",
    "               max_steps=500,                # Number of steps to train\n",
    "               scaler_type='standard',       # Type of scaler to normalize data\n",
    "               encoder_hidden_size=64,       # Defines the size of the hidden state of the LSTM\n",
    "               decoder_hidden_size=64,),     # Defines the number of hidden units of each layer of the MLP decoder\n",
    "          NHITS(h=horizon,                   # Forecast horizon\n",
    "                input_size=2 * horizon,      # Length of input sequence\n",
    "                max_steps=100,               # Number of steps to train\n",
    "                n_freq_downsample=[2, 1, 1]) # Downsampling factors for each stack output\n",
    "          ]\n",
    "nf = NeuralForecast(models=models, freq='M')\n",
    "nf.fit(df=Y_df)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c78f0f4e",
   "metadata": {},
   "source": [
    "Y_hat_df = nf.predict() #obtain the h forecasts after the training data Y_df."
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "6b24fc72",
   "metadata": {},
   "source": [
    "Y_hat_df"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f0536e51",
   "metadata": {},
   "source": [
    "Y_hat_df = Y_hat_df.reset_index()\n",
    "Y_hat_df.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "98b19519",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a3b6a144",
   "metadata": {},
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize = (20, 7))\n",
    "plot_df = pd.concat([Y_df, Y_hat_df]).set_index('ds') # Concatenate the train and forecast dataframes\n",
    "plot_df[['y', 'LSTM', 'NHITS']].plot(ax=ax, linewidth=2)\n",
    "\n",
    "ax.set_title('AirPassengers Forecast', fontsize=22)\n",
    "ax.set_ylabel('Monthly Passengers', fontsize=20)\n",
    "ax.set_xlabel('Timestamp [t]', fontsize=20)\n",
    "ax.legend(prop={'size': 15})\n",
    "ax.grid()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "aa1297c0",
   "metadata": {},
   "source": [
    "## Data Inputs"
   ]
  },
  {
   "cell_type": "code",
   "id": "2a30c2c9",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "from datasetsforecast.m3 import M3"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "89065072",
   "metadata": {},
   "source": [
    "Y_df, *_ = M3.load('./data', group='Yearly')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "533f57af",
   "metadata": {},
   "source": [
    "Y_df.groupby('unique_id').head(2)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "e2eabfe1",
   "metadata": {},
   "source": [
    "## Exogenous Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf3721b5",
   "metadata": {},
   "source": [
    "* Static exogenous variables: The static exogenous variables carry time-invariant information for each time series. When the model is built with global parameters to forecast multiple time series, these variables allow sharing information within groups of time series with similar static variable levels. Examples of static variables include designators such as identifiers of regions, groups of products, etc.\n",
    "\n",
    "* Historic exogenous variables: This time-dependent exogenous variable is restricted to past observed values. Its predictive power depends on Granger-causality, as its past values can provide significant information about future values of the target variable y\n",
    "\n",
    "* Future exogenous variables: In contrast with historic exogenous variables, future values are available at the time of the prediction. Examples include calendar variables, weather forecasts, and known events that can cause large spikes and dips such as scheduled promotions."
   ]
  },
  {
   "cell_type": "code",
   "id": "ebf8fd57",
   "metadata": {},
   "source": [
    "df = pd.read_csv('https://datasets-nixtla.s3.amazonaws.com/EPF_FR_BE.csv')\n",
    "df['ds'] = pd.to_datetime(df['ds'])\n",
    "df.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "5dad6a8e",
   "metadata": {},
   "source": [
    "plt.figure(figsize=(15,5))\n",
    "plt.plot(df[df['unique_id']=='FR']['ds'], df[df['unique_id']=='FR']['y'])\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Price [EUR/MWh]')\n",
    "plt.grid()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "98c11973",
   "metadata": {},
   "source": [
    "static_df = pd.read_csv('https://datasets-nixtla.s3.amazonaws.com/EPF_FR_BE_static.csv')\n",
    "static_df.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f7581456",
   "metadata": {},
   "source": [
    "from neuralforecast.auto import NHITS, BiTCN\n",
    "from neuralforecast.core import NeuralForecast\n",
    "\n",
    "import logging\n",
    "logging.getLogger(\"pytorch_lightning\").setLevel(logging.WARNING)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "aebc50e6",
   "metadata": {},
   "source": [
    "horizon = 24 # day-ahead daily forecast\n",
    "models = [NHITS(h = horizon,\n",
    "                input_size = 5*horizon,\n",
    "                futr_exog_list = ['gen_forecast', 'week_day'], # <- Future exogenous variables\n",
    "                hist_exog_list = ['system_load'], # <- Historical exogenous variables\n",
    "                stat_exog_list = ['market_0', 'market_1'], # <- Static exogenous variables\n",
    "                scaler_type = 'robust'),\n",
    "          BiTCN(h = horizon,\n",
    "                input_size = 5*horizon,\n",
    "                futr_exog_list = ['gen_forecast', 'week_day'], # <- Future exogenous variables\n",
    "                hist_exog_list = ['system_load'], # <- Historical exogenous variables\n",
    "                stat_exog_list = ['market_0', 'market_1'], # <- Static exogenous variables\n",
    "                scaler_type = 'robust',\n",
    "                ),                \n",
    "                ]\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b5ba9a55",
   "metadata": {},
   "source": [
    "nf = NeuralForecast(models=models, freq='H')\n",
    "nf.fit(df=df,\n",
    "       static_df=static_df)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "43532033",
   "metadata": {},
   "source": [
    "futr_df = pd.read_csv('https://datasets-nixtla.s3.amazonaws.com/EPF_FR_BE_futr.csv')\n",
    "futr_df['ds'] = pd.to_datetime(futr_df['ds'])\n",
    "futr_df.head()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d90d844e",
   "metadata": {},
   "source": [
    "Y_hat_df = nf.predict(futr_df=futr_df)\n",
    "Y_hat_df.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "8c654104",
   "metadata": {},
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plot_df = df[df['unique_id']=='FR'].tail(24*5).reset_index(drop=True)\n",
    "Y_hat_df = Y_hat_df.reset_index(drop=False)\n",
    "Y_hat_df = Y_hat_df[Y_hat_df['unique_id']=='FR']\n",
    "\n",
    "plot_df = pd.concat([plot_df, Y_hat_df ]).set_index('ds') # Concatenate the train and forecast dataframes\n",
    "\n",
    "plot_df[['y', 'NHITS', 'BiTCN']].plot(linewidth=2)\n",
    "plt.axvline('2016-11-01', color='red')\n",
    "plt.ylabel('Price [EUR/MWh]', fontsize=12)\n",
    "plt.xlabel('Date', fontsize=12)\n",
    "plt.grid()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "f67e7b0c",
   "metadata": {},
   "source": [
    "## Time Series Scaling\n",
    "\n",
    "The Neuralforecast library integrates two types of temporal scaling:\n",
    "\n",
    "* Time Series Scaling: scaling each time series using all its data on the train set before start training the model. This is done by using the \"local_scaler_type\" parameter of the Neuralforecast core class. local_scaler_type parameter is used to specify the type of scaling to be used. In this case, we will use standard, which scales the data to have zero mean and unit variance.Other supported scalers are minmax, robust, robust-iqr, minmax, and boxcox.\n",
    "\n",
    "* Window scaling (TemporalNorm): scaling each input window separetly for each element of the batch at every training iteration. This is done by using the \"scaler_type\" parameter of each model class. Temporal normalization is specified by the scaler_type argument. Currently, it is only supported for Windows-based models (NHITS, NBEATS, MLP, TimesNet, and all Transformers)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b474560",
   "metadata": {},
   "source": [
    "## Hyperparameter Optimization\n",
    "The main steps of hyperparameter tuning are:\n",
    "\n",
    "* 1- Define training and validation sets.\n",
    "* 2- Define search space.\n",
    "* 3- Sample configurations with a search algorithm, train models, and evaluate them on the validation set.\n",
    "* 4- Select and store the best model."
   ]
  },
  {
   "cell_type": "code",
   "id": "20f31f76",
   "metadata": {},
   "source": [
    "from neuralforecast.utils import AirPassengersDF\n",
    "\n",
    "Y_df = AirPassengersDF\n",
    "Y_df.head()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e8cb1712",
   "metadata": {},
   "source": [
    "#complete"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "a5c502ca",
   "metadata": {},
   "source": [
    "## End to End Walkthrough\n",
    "Outline:\n",
    "\n",
    "* 1- Install packages.\n",
    "* 2- Read the data.\n",
    "* 3- Explore the data.\n",
    "* 4- Train many models globally for the entire dataset.\n",
    "* 5- Evaluate the model’s performance using cross-validation.\n",
    "* 6- Select the best model for every unique time series."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82dbd8a7",
   "metadata": {},
   "source": [
    "The input to NeuralForecast is always a data frame in long format with three columns: unique_id, ds and y:"
   ]
  },
  {
   "cell_type": "code",
   "id": "dc7a17fd",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "\n",
    "Y_df = pd.read_parquet('https://datasets-nixtla.s3.amazonaws.com/m4-hourly.parquet')\n",
    "\n",
    "Y_df.head()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c14a65af",
   "metadata": {},
   "source": [
    "uids = Y_df['unique_id'].unique()[:10] # Select 10 ids to make the example faster\n",
    "Y_df = Y_df.query('unique_id in @uids').reset_index(drop=True)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "010e952f",
   "metadata": {},
   "source": [
    "from statsforecast import StatsForecast\n",
    "\n",
    "StatsForecast.plot(Y_df, engine='matplotlib')\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "132a136c",
   "metadata": {},
   "source": [
    "from ray import tune\n",
    "\n",
    "from neuralforecast import NeuralForecast\n",
    "from neuralforecast.auto import AutoNHITS, AutoLSTM\n",
    "from neuralforecast.losses.pytorch import MQLoss"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "19fc66c4",
   "metadata": {},
   "source": [
    "config_nhits = {\n",
    "    \"input_size\": tune.choice([48, 48*2, 48*3]),              # Length of input window\n",
    "    \"start_padding_enabled\": True,\n",
    "    \"n_blocks\": 5*[1],                                              # Length of input window\n",
    "    \"mlp_units\": 5 * [[64, 64]],                                  # Length of input window\n",
    "    \"n_pool_kernel_size\": tune.choice([5*[1], 5*[2], 5*[4],         \n",
    "                                      [8, 4, 2, 1, 1]]),            # MaxPooling Kernel size\n",
    "    \"n_freq_downsample\": tune.choice([[8, 4, 2, 1, 1],\n",
    "                                      [1, 1, 1, 1, 1]]),            # Interpolation expressivity ratios\n",
    "    \"learning_rate\": tune.loguniform(1e-4, 1e-2),                   # Initial Learning rate\n",
    "    \"scaler_type\": tune.choice([None]),                             # Scaler type\n",
    "    \"max_steps\": tune.choice([1000]),                               # Max number of training iterations\n",
    "    \"batch_size\": tune.choice([1, 4, 10]),                          # Number of series in batch\n",
    "    \"windows_batch_size\": tune.choice([128, 256, 512]),             # Number of windows in batch\n",
    "    \"random_seed\": tune.randint(1, 20),                             # Random seed\n",
    "}\n",
    "\n",
    "config_lstm = {\n",
    "    \"input_size\": tune.choice([48, 48*2, 48*3]),              # Length of input window\n",
    "    \"encoder_hidden_size\": tune.choice([64, 128]),            # Hidden size of LSTM cells\n",
    "    \"encoder_n_layers\": tune.choice([2,4]),                   # Number of layers in LSTM\n",
    "    \"learning_rate\": tune.loguniform(1e-4, 1e-2),             # Initial Learning rate\n",
    "    \"scaler_type\": tune.choice(['robust']),                   # Scaler type\n",
    "    \"max_steps\": tune.choice([500, 1000]),                    # Max number of training iterations\n",
    "    \"batch_size\": tune.choice([1, 4]),                        # Number of series in batch\n",
    "    \"random_seed\": tune.randint(1, 20),                       # Random seed\n",
    "}\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "4c3a24fc",
   "metadata": {},
   "source": [
    "nf = NeuralForecast(\n",
    "    models=[\n",
    "        AutoNHITS(h=48, config=config_nhits, loss=MQLoss(), num_samples=5),\n",
    "        AutoLSTM(h=48, config=config_lstm, loss=MQLoss(), num_samples=2),\n",
    "    ],\n",
    "    freq=1\n",
    ")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b8c37d62",
   "metadata": {},
   "source": [
    "nf.fit(df=Y_df)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "45de7ebe",
   "metadata": {},
   "source": [
    "fcst_df = nf.predict()\n",
    "fcst_df.columns = fcst_df.columns.str.replace('-median', '')\n",
    "fcst_df.head()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "8f796a59",
   "metadata": {},
   "source": [
    "# Plot to unique_ids and some selected models\n",
    "StatsForecast.plot(Y_df, fcst_df, models=[\"AutoLSTM\"], unique_ids=[\"H107\", \"H104\"], level=[80, 90], engine='matplotlib')\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "271e9770",
   "metadata": {},
   "source": [
    "cv_df = nf.cross_validation(Y_df, n_windows=2)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "50d64316",
   "metadata": {},
   "source": [
    "cv_df.columns = cv_df.columns.str.replace('-median', '')\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "8474165c",
   "metadata": {},
   "source": [
    "cv_df.head()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a04edf8b",
   "metadata": {},
   "source": [
    "for cutoff in cv_df['cutoff'].unique():\n",
    "    StatsForecast.plot(\n",
    "        Y_df, \n",
    "        cv_df.query('cutoff == @cutoff').drop(columns=['y', 'cutoff']), \n",
    "        max_insample_length=48 * 4, \n",
    "        unique_ids=['H185'],\n",
    "        engine='matplotlib'\n",
    "    )\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "06cf0638",
   "metadata": {},
   "source": [
    "from datasetsforecast.losses import mse, mae, rmse\n",
    "from datasetsforecast.evaluation import accuracy\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "357e371e",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "evaluation_df = accuracy(cv_df, [mse, mae, rmse], agg_by=['unique_id'])\n",
    "evaluation_df['best_model'] = evaluation_df.drop(columns=['metric', 'unique_id']).idxmin(axis=1)\n",
    "evaluation_df.head()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "51e334a9",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
